<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Sameer Manek</title>
 <link href="http://sameermanek.github.io/atom.xml" rel="self"/>
 <link href="http://sameermanek.github.io/"/>
 <updated>2017-07-20T11:12:52-04:00</updated>
 <id>http://sameermanek.github.io</id>
 <author>
   <name>Sameer Manek</name>
   <email></email>
 </author>

 
 <entry>
   <title>Identifying 'good' photographs</title>
   <link href="http://sameermanek.github.io//2017/07/15/photo-quality/"/>
   <updated>2017-07-15T00:00:00-04:00</updated>
   <id>http://sameermanek.github.io/2017/07/15/photo-quality</id>
   <content type="html">&lt;p&gt;I use deep learning to try identify which pictures I’ve taken are good (using a loosely defined version of ‘good’). The resultant convolutional neural network (CNN) performs well (0.81 AUC), helped me identify some of my own biases, and seems like a fairly interesting start for more analysis. &lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Applying deep learning to image recognition is not new; more recently though, deep learning has been used to rate image aesthetics (e.g., &lt;a href=&quot;http://ieeexplore.ieee.org/document/7243357/&quot;&gt;Lu&lt;/a&gt;) and very recently, there’s been an uptick in using deep learning algorithms to replace photographers (e.g., &lt;a href=&quot;https://arxiv.org/abs/1707.03491&quot;&gt;Fang&lt;/a&gt; and &lt;a href=&quot;https://www.kickstarter.com/projects/2092430307/arsenal-the-intelligent-camera-assistant-0&quot;&gt;Arsenal&lt;/a&gt;[1]). I attempted something similar, albeit hugely simplified myself, largely as a deep learning exercise that was more interesting than tweaking the architecture of existing &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10&lt;/a&gt; recognition &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;CNNs&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Using mostly my own photographs, my system was able to identify the photos that I decided were ‘good’ (read: not terrible). The results were ~71% accurate (with an unbalanced test set) and had an AUC of 0.81, as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photo-quality/test_auc_1.png&quot; alt=&quot;AUC of test set&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;
&lt;p&gt;Here’s where things start to get interesting. As a (digital hoarder) well-organized individual, I maintained copies of most of the photos I’ve taken on recent trips, having organized them into ‘good’ and ‘bad’ folders. I’m not a good photographer, so my usual method is to capture a bunch and hope that I get a few decent photos by sheer chance. I pulled the photographs from three trips I’ve taken within the past three years. This netted me 1,849 photos, 438 of which I thought were ‘good’. I incorporated an additional 111 photos taken by others on these same trips to net 549 ‘good’ photos.&lt;/p&gt;

&lt;h3 id=&quot;data-issues&quot;&gt;Data Issues&lt;/h3&gt;

&lt;h4 id=&quot;samples&quot;&gt;Samples&lt;/h4&gt;
&lt;p&gt;This is a pretty small data set for deep learning applications. To account for this, I strove to minimize the total number of tunable parameters, and ended up with ~47k (compared to &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;~60M for AlexNet and ~140M for VGGNet&lt;/a&gt;). While this reduced my maximum potential performance and flexibility, it helped deal with the relatively low number of samples.&lt;/p&gt;

&lt;h4 id=&quot;unbalanced-classes&quot;&gt;Unbalanced Classes&lt;/h4&gt;
&lt;p&gt;Roughly 3/4 of the images I had were classified as ‘bad’. I sampled from the training set with replacement and oversampled the ‘good’ photos to account for this.&lt;/p&gt;

&lt;h4 id=&quot;varying-image-sizes&quot;&gt;Varying Image Sizes&lt;/h4&gt;
&lt;p&gt;I resized images to 480px wide, but maintained their original aspect ratio. This caused two issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The model would have to handle differenly sized images. I dealt with this by introducing a &lt;a href=&quot;https://arxiv.org/pdf/1312.4400.pdf&quot;&gt;global average pooling&lt;/a&gt; layer after before a fully connected (output) layer. I don’t think this is an entirely conventional use of the global average pooling, but architecturally allowed me to handle various-sized images.&lt;/li&gt;
  &lt;li&gt;As a pure data-formatting issue, the various sizes constrained how I constructed batches. That is, all batches would need to be of the same dimensions. I randomly sampled images from the training set with replacement to construct batches, but that means that many batches ended up being repeats of the same image (e.g., I only had 1 480x544 image in my training set). To prevent this oversampling from influencing the model too much, I used a relatively small batch size (4 photos per batch). &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;biased-data-set&quot;&gt;Biased Data Set&lt;/h4&gt;
&lt;p&gt;I labeled all the images myself, prior to starting this project, over a relatively short amount of time. The ‘good’ images were generally corrected/adjusted in lightroom, whereas the ‘bad’ images generally were not. This created a biased data set, which certainly makes this model less generally applicable. It does make the problem much easier though – it is presumably easier to systematically identify my preferences than some abstract notion of ‘good’ vs ‘bad’ photos. &lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;The model is a fairly straightforward convolutional neural network (CNN). I chose to use several convolution layers to allow the network to have a broader (effective) visual field without increasing the number of parameters too much. &lt;/p&gt;

&lt;p&gt;Sorry for the crummy screenshot; I’m working to incorporate a more clear diagram.
&lt;img src=&quot;/assets/photo-quality/model_architecture_stand_in.png&quot; alt=&quot;Model Architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance-and-discussion&quot;&gt;Performance and Discussion&lt;/h2&gt;
&lt;p&gt;The plot below shows how the performance changed over time. Because of the small batch size, I’m only showing the moving average of the training set performance; otherwise the data would be far too noisy. As you can see, the validation set performance bounces around quite a bit (likely due to training on small batches that weren’t simple random samples). &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photo-quality/performance.png&quot; alt=&quot;Performance by batch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The performance on the validation data set starts to plateau around batch 10,000, but appears to still be improving with more batches; i.e., this model is probably underfit. The performance on the training set is improving, and would be expected to continue to improve (as the model would eventually start to overfit). &lt;/p&gt;

&lt;p&gt;As a validation, I manually inspected the test set performance. I expected it to identify my own biases (e.g., landscapes &amp;gt; portraits, wide angle &amp;gt; tight angle, a solid horizon at the bottom 1/3rd, etc). Some of these are shown below; the first thirteen are high-scoring images, the last twelve are low-scoring images; images outlined in red are misclassified. I redacted potentially identifiable faces for privacy and shrank the images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/photo-quality/test_grid_5_5.jpg&quot; alt=&quot;Sample Images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model appears to have picked up on some of my biases (many of which were unknown to myself). Clearly I’m biased towards blue, white, and green, I seem to prefer non-flat horizons, and at least a few people in the photo. &lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;I worked on this project entirely for fun, so there are still quite a few areas I’d like to explore. I’m most interested in better understanding what the network is ‘thinking’; i.e., whether it has identified simple heuristics (like color and regions of contrast). To that end, I’m planning to apply deep dream to this to try and better understand what it’s seeing (and whether I can transform ‘bad’ photos into ‘good’ ones that simply). &lt;/p&gt;

&lt;p&gt;There’s been some interesting work done on broader photo sets (e.g., the &lt;a href=&quot;https://github.com/shubhamchaudhary/aesthetics&quot;&gt;Ava/DPChallenge photo libraries&lt;/a&gt;), and I am curious to try those out – to try applying this trained model to those data sets, and conversely train on those data sets to evaluate my own pictures. &lt;/p&gt;

&lt;p&gt;I performed this work locally on CPU using a combination of R, Keras, and Tensorflow. I had very little free memory, so I kept most of the data on disk and found that it wasn’t a huge hit to performance. To work with these larger data sets, I’ll likely move to a cloud GPU-based solution (e.g., AWS) and maybe even try to use spot instances, which would make the engineering somewhat more of a challenge (i.e., moving to/from disk will become a significant handicap, and it’ll need to be more fault tolerant/scalable).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;[1]From what I can tell, &lt;a href=&quot;https://witharsenal.com/blog/choosing-camera-settings/&quot;&gt;Arsenal&lt;/a&gt; uses deep learning to recognize (objects in) the scene and then applies a set of expert-defined parameters in response to water/forests/etc. If well done, that’s certainly nothing to sneeze at, but it sounds like a different implementation of built-in &lt;a href=&quot;http://docs.esupport.sony.com/dvimag/DSCRX100_guide/en/contents/02/03/02/02.html&quot;&gt;scene recognition auto mode&lt;/a&gt; that already exists in many cameras. Happy to be corrected if I’m wrong though.&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FCC Comments</title>
   <link href="http://sameermanek.github.io//2017/05/15/fcc/"/>
   <updated>2017-05-15T00:00:00-04:00</updated>
   <id>http://sameermanek.github.io/2017/05/15/fcc</id>
   <content type="html">&lt;p&gt;Based on an extremely non-scientific analysis, there seem to be two astroturf anti net neutrality campaigns, along with one potentially organic net neutrality campaign (CFIF)&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Since late April, the FCC has been taking comments on &lt;a href=&quot;https://www.fcc.gov/ecfs/search/filings?proceedings_name=17-108&amp;amp;sort=date_disseminated,DESC&quot;&gt;Docket 17-108 Restoring Internet Freedom&lt;/a&gt;. This has garnered over 1M comments, a large portion of which were in the past week, following &lt;a href=&quot;https://www.youtube.com/watch?v=92vuuZt7wak&quot;&gt;John Oliver’s call to action on May 7th&lt;/a&gt;. The FCC website was so overwhelmed that it &lt;a href=&quot;https://www.engadget.com/2017/05/08/fcc-website-down-net-neutrality-last-week-tonight-john-oliver/&quot;&gt;went offline&lt;/a&gt; for some time immediately after the segment aired. 
## Content Concentration&lt;/p&gt;

&lt;p&gt;There’s mounting evidence that some of these comments aren’t organic users; in fact, more than half of all comments are verbatim repeats of 3 anti net neutrality comments, all found on anti net neutrality websites. The top comments are shown below (some truncated for clarity/spacing)
&lt;img src=&quot;/assets/fcc/table.png&quot; alt=&quot;Table of common comments&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Or, to make &lt;a href=&quot;http://www.infovis-wiki.net/index.php/Data-Ink_Ratio&quot;&gt;Tufte truly irate&lt;/a&gt;, an entirely superfluous ink-heavy graphic
&lt;img src=&quot;/assets/fcc/totals.png&quot; alt=&quot;Graph of common comments&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;timing&quot;&gt;Timing&lt;/h2&gt;
&lt;p&gt;I think this is the interesting piece. A timeline of the hourly number comments posted for the top three most common comments, alongside a view of the other (apparently organic) comments posted in green. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/fcc/times.png&quot; alt=&quot;Timeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The green shows a pattern roughly as expected – an initial gap due to the fcc.gov outage followed by a peak the following evening and a decay (with a somewhat circadian rhythm to it), along with a few gaps, likely corresponding to when the fcc website was overloaded/slow. &lt;/p&gt;

&lt;p&gt;The red and blue comments (from the Taxpayer Protection Alliance and freeourinternet.org) do not follow this or any reasonable pattern. To quantify it, their posting volume is highly correlated with each other (pearson correlation of 0.7 for the week shown above) and uncorrelated with the baseline ‘Other comments’ (between 0.0 and 0.1). &lt;/p&gt;

&lt;p&gt;The behavior of the CFIF message looks almost human-like, at least on this hourly basis. There appear to be gaps/trickles whent he website was having issues and the ramping up and down seems similar in pattern to the baseline ‘Other comments’. It is most correlated with the baseline ‘Other comments’ (correlation coefficient  of 0.5), and the hourly autocorrelation coefficient is similar to the ‘Other comments’ (highly autocorrelated at the first and second lags). While the distribution is very different (when the bulk of the comments were made), most of the ‘Other comments’ were made soon after the John Oliver segment. &lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While the ‘Obama’ and ‘Soros’ messages seem pretty fishy, I woulnd’t be surprised if the CFIF message was genuinely posted by anti net-neutrality advocates (although likely through some intermediary mechanism, like a form on another webiste). &lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;This is entirely unscientific. There’s a decent chance that all of this is incorrect and I make no claims about the accuracy.&lt;/li&gt;
  &lt;li&gt;The ‘sources’ of these comments are guesses, at best, based on a combination of reddit and googling. &lt;/li&gt;
  &lt;li&gt;The FCC is surely tracking more detailed behavior and hopefully will choose to identify which comments are true and which were posted without the listed commenter’s knowledge/permission.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.theverge.com/2017/5/10/15610744/anti-net-neutrality-fake-comments-identities&quot;&gt;actual&lt;/a&gt; &lt;a href=&quot;http://www.zdnet.com/article/a-bot-is-flooding-the-fccs-website-with-fake-anti-net-neutrality-comments/&quot;&gt;reporting&lt;/a&gt; being done is way more important than this very high-level summarization with numerous assumptions. I wish they’d note which suspect comments they were following up with (i.e., whether they’re the CFIF or one of the other comments), but either way their actual research is valuable.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>A/B Testing for Scammers</title>
   <link href="http://sameermanek.github.io//2016/02/23/ab-testing/"/>
   <updated>2016-02-23T00:00:00-05:00</updated>
   <id>http://sameermanek.github.io/2016/02/23/ab-testing</id>
   <content type="html">&lt;p&gt;I don’t know whether to feel happy or sad that scammers are A/B testing poorly. I recently received two emails, that were obviously part of an experiment done poorly. They’re shown below, but a few notes for a potential future scammer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/abtest.png&quot; alt=&quot;Scammer Emails&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;test-subjects&quot;&gt;Test Subjects&lt;/h2&gt;

&lt;p&gt;It isn’t strictly wrong to use the same user for both arms of the A/B test (although then you’d need to test whether the treatment order matters), but for something like this, it is generally better to focus on a two-sample test. Anecdotally, I don’t think people would respond to one and not the other purely as a function of the messaging. Therefore, you should only send each user one treatment within a given time period. While you’d sacrifice some statistical power using a two-sample test rather than a paired test, you’d probably have a more accurate measurement overall. &lt;/p&gt;

&lt;h2 id=&quot;treatments&quot;&gt;Treatments&lt;/h2&gt;

&lt;p&gt;Here the scammers changed the amounts in the title, which is a great component to experiment on; they’re testing whether odd numbers are more likely to generate the target behavior than even numbers. Buzzfeed has found &lt;a href=&quot;https://medium.com/i-data/29-reasons-youre-reading-this-article-fbf4671327e3#.4d3g5ve8h&quot;&gt;something similar&lt;/a&gt; with their listicles. &lt;/p&gt;

&lt;p&gt;My primary concern with their treatments are that the amounts and currencies differ by quite a bit. It is one thing to test whether someone reacts better/worse to an even/unusual amount, but it is another to test whether the sweet spot is ~$10M or ~$1M, and whether they’d react differently to USD vs GBP. &lt;/p&gt;

&lt;p&gt;Another issue related to the treatment: they used different numbers in the body of the text. This seems like a simple oversight, but means that any response they detect may be a function of the inconsistent text, thus potentially nullifying any results from this experiment. &lt;/p&gt;

&lt;h2 id=&quot;dependent-variable&quot;&gt;Dependent Variable&lt;/h2&gt;

&lt;p&gt;A little more difficult to notice, but here the scammers would have a difficult time detecting whether there was any response. I think they should have used a subtly different email address for each of the messages, that way they could keep track of responses, although maybe they’re assuming the user will actually just click ‘reply’ and add the email addresses in themselves.&lt;/p&gt;

&lt;h2 id=&quot;confounding-variables&quot;&gt;Confounding Variables&lt;/h2&gt;

&lt;p&gt;Finally, these emails were sent at pretty different times. While it is possible that they’re controlling for this (through randomization of when the emails are sent), there’s quite a bit of evidence that the &lt;a href=&quot;http://www.wordstream.com/blog/ws/2014/09/04/best-time-to-send-email-campaign&quot;&gt;timing of emails matters&lt;/a&gt;. By sending one email in mid-morning and the other in the middle of the night (in my time zone, at least), this confounding variable may cause erroneous results. &lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While the use of a randomized control trial is laudable, its misuse can misinform and even lead to the wrong conclusions. As more non-specialists use RCTs (the apocalypse must be near – they’re even teaching these things to MBAs nowadays), these errors are more likely to occur. Look to a statistician or a reliable online resource to make sure you don’t make these simple mistakes. &lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Clearly this post is meant as a satire. I am not a lawyer and don’t condone trying to scam anyone. I do not take responsibility for the consequences of this post. I mostly meant to help people understand a little about what makes A/B tests useful and useless.&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dr. Uber or How I Learned to Stop Worrying and Love the Employees</title>
   <link href="http://sameermanek.github.io//2015/09/05/uber-employees/"/>
   <updated>2015-09-05T00:00:00-04:00</updated>
   <id>http://sameermanek.github.io/2015/09/05/uber-employees</id>
   <content type="html">&lt;p&gt;For the past few months, the &lt;a href=&quot;http://www.newyorker.com/magazine/2015/07/06/gigs-with-benefits&quot;&gt;press&lt;/a&gt; has been &lt;strike&gt;focused&lt;/strike&gt; fixated on whether Uber will be forced to reclassify drivers from contractors to employees, it has ignored a bigger question: what would be the effect of Uber employing millions of drivers?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://recode.net/2015/07/14/uber-could-have-to-pay-an-additional-209-million-to-reclassify-its-drivers-in-california/&quot;&gt;re/code&lt;/a&gt; analyzed Uber’s drivers in California and estimated that employment would cost them an incremental $200 million compared to Uber’s current California revenue of $250M (Uber takes 20% of all UberX transactions and gives the other 80% to the drivers). This could be disastrous to Uber’s current business model and would probably force them to increase prices to users and reduce hourly pay to drivers.  &lt;/p&gt;

&lt;p&gt;Nonetheless, this might be the best possible outcome for Uber. &lt;/p&gt;

&lt;h2 id=&quot;network-effects&quot;&gt;Network Effects&lt;/h2&gt;

&lt;p&gt;Uber’s current business model is heavily reliant on network effects (commonly called a virtuous cycle), David Sacks tweeted quite eloquently. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/davidsacks/status/475073311383105536&quot;&gt;&lt;img src=&quot;/assets/david sacks twitter.png&quot; alt=&quot;David Sacks&#39; virtuous cycle&quot; /&gt;&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://abovethecrowd.com/2014/07/11/how-to-miss-by-a-mile-an-alternative-look-at-ubers-potential-market-size/&quot;&gt;Bill Gurley&lt;/a&gt;, an early investor in Uber, described in more detail why these network effects justify Uber’s valuation and dominance.&lt;/p&gt;

&lt;p&gt;Yet, &lt;a href=&quot;http://therideshareguy.com/how-to-drive-for-uber-and-lyft-at-the-same-time/&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;https://www.quora.com/Ride-Sharing-Company-Comparisons/Can-you-be-a-Lyft-UberX-and-Sidecar-driver-at-the-same-time&quot;&gt;drivers&lt;/a&gt; &lt;a href=&quot;http://money.cnn.com/2014/08/04/technology/uber-lyft/&quot;&gt;switch&lt;/a&gt; &lt;a href=&quot;https://pando.com/2014/12/03/uber-vs-lyft-a-former-driver-compares-the-two-services/&quot;&gt;between&lt;/a&gt; Uber and Lyft or use both simultaneously. The reason is fairly well understood by economists: low costs of multi-homing on both sides of the marketplace. Because it is almost trivially easy to have accounts and apps for both Uber and Lyft (as a driver or as a rider), ridesharing is not currently a winner-take-all market. In fact, Uber and Lyft could coexist the same way as different credit card networks, ad-supported streaming video services, and instant messenger apps continue to exist. This concept is known as &lt;a href=&quot;http://www.econstor.eu/bitstream/10419/26118/1/555836088.PDF&quot;&gt;multi-homing&lt;/a&gt; and is fairly well documented in literature. &lt;/p&gt;

&lt;p&gt;If Uber and Lyft are forced to employ their drivers, it would be impossible for drivers to work for both simultaneously and very difficult to switch from day to day. Therefore, multi-homing would be essentially impossible for drivers – this would strengthen the already-extant network effects and change ridesharing into a winner-take-all market, which would help the incumbent, Uber.  &lt;/p&gt;

&lt;p&gt;In short, if the gig-economy shifts from a contractor- to employee- based model, the largest players – Uber, Postmates, Instacart, et al – would be protected from smaller potential competitors. &lt;/p&gt;

&lt;p&gt;This might be the best possible thing that could happen to Uber.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>To log or not to log Part 1</title>
   <link href="http://sameermanek.github.io//2015/03/28/log-scales-1/"/>
   <updated>2015-03-28T00:00:00-04:00</updated>
   <id>http://sameermanek.github.io/2015/03/28/log-scales-1</id>
   <content type="html">&lt;p&gt;I’ve noticed a frustrating phenomenon: the use of linear scales where a log scale would be much more appropriate. While people are more conscientious of deceptive charts now more than ever, misleading scales continue to be an issue. This is my brief rant against them. &lt;/p&gt;

&lt;p&gt;While I have no idea whether this choice of scales is intentional or oversight, increasing awareness of users and makers seems like a suitable first step regardless. The purpose of this post and the following (short/occasional) series of posts is to give good reasons and cases in which log scales should be used.&lt;/p&gt;

&lt;h2 id=&quot;exponential-growth&quot;&gt;Exponential Growth&lt;/h2&gt;
&lt;p&gt;This is the most obvious case for using a log scale rather than a linear scale. It seems like a forgone conclusion that when plotting a timeseries of values which naturally grow exponentially, one should use a log scale. &lt;/p&gt;

&lt;h3 id=&quot;singapore&quot;&gt;Singapore&lt;/h3&gt;
&lt;p&gt;Nonetheless, in the past week alone I’ve seen &lt;a href=&quot;http://www.hbs.edu/faculty/units/bgie/Pages/default.aspx&quot; title=&quot;BGIE Department, where the flaw was found&quot;&gt;Harvard Business School&lt;/a&gt; and &lt;a href=&quot;http://www.economist.com/&quot;&gt;The Economist&lt;/a&gt; inappropriately use linear scales. Both extremely well-respected and generally unbiased, they nonetheless presented quite misleading views.&lt;/p&gt;

&lt;p&gt;For example, the &lt;a href=&quot;http://www.economist.com/blogs/graphicdetail/2015/03/lee-kuan-yews-singapore&quot; title=&quot;Lee Kuan Yew&#39;s Singapore&quot;&gt;Economist presented a graph&lt;/a&gt; quite like the one below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gdp_linear.png&quot; alt=&quot;Plot of GDP per Capita&quot; /&gt; &lt;/p&gt;

&lt;p&gt;They concluded that this graph clearly showed how dramatic Singapore’s rise has been. While I agree with the conclusion, their visualization made it nearly impossible to tell that China’s rise has been similarly outstanding (even more so in the past 2 decades), or that post-war Japan was similarly meteoric. A better graph would have been the one below, with a log scale. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gdp_log.png&quot; alt=&quot;Plot of GDP per Capita with a Log Scale&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;nigeria&quot;&gt;Nigeria&lt;/h3&gt;
&lt;p&gt;In a nearly identical situation, a professor of mine presented a graph like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gdp_nigeria_linear.png&quot; alt=&quot;Plot of GDP per Capita of Nigeria&quot; /&gt;&lt;/p&gt;

&lt;p&gt;She claimed that this showed how Nigeria was struggling against the other BRINCS countries (Brazil, Russia, India, Nigeria, China, South Africa). To potentially oversimplify a long and complex history, Nigeria has been seen as a future economic powerhouse for the past 50 years, but various political, regional, and social issues have interfered. However, there has been substantial growth over the past decade and a half under the most recent series of leaders. The graph presented barely showed an uptick in GDP per capita, when in reality, GDP growth in the 2000s has been over 3%, in line with some other resource-dependent BRINCS countries (Brazil and South Africa). I think this point would have been clear had a log scale been used:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gdp_nigeria_log.png&quot; alt=&quot;Plot of GDP per Capita of Nigeria&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Exponential growth is the most straightforward case. While both the examples above were related to money, it truly applies to any case of exponential growth: money, population, etc. &lt;/p&gt;

&lt;p&gt;The code used to generate the graphs above is available &lt;a href=&quot;/assets/log_scales_1.R&quot;&gt;here&lt;/a&gt;. The data for this post came from &lt;a href=&quot;http://www.ggdc.net/maddison/maddison-project/home.htm&quot;&gt;The Maddison Project&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Data-Based Analysis of Hacker News</title>
   <link href="http://sameermanek.github.io//2015/02/09/hn-analysis/"/>
   <updated>2015-02-09T00:00:00-05:00</updated>
   <id>http://sameermanek.github.io/2015/02/09/hn-analysis</id>
   <content type="html">&lt;p&gt;Or: 2048 Con, asking Edward.&lt;/p&gt;

&lt;p&gt;After spending a little too much time reading Hacker News, I decided to take a more systematic look at what ends up on HN and what does well. This analysis inspired the alternate title – read on to find out more.&lt;/p&gt;

&lt;p&gt;As a quick reminder, &lt;a href=&quot;https://news.ycombinator.com&quot;&gt;Hacker News&lt;/a&gt; is a website about a wide-range of subjects – everything from &lt;a href=&quot;https://news.ycombinator.com/item?id=9015092&quot;&gt;philosophy&lt;/a&gt; and &lt;a href=&quot;https://news.ycombinator.com/item?id=8940723&quot;&gt;design&lt;/a&gt; to &lt;a href=&quot;https://news.ycombinator.com/item?id=8980047&quot;&gt;job postings&lt;/a&gt; and &lt;a href=&quot;https://news.ycombinator.com/item?id=9018247&quot;&gt;programming languages&lt;/a&gt;. Hacker News, or as my classmates called it “that hacker website”, is frequently compared to &lt;a href=&quot;http://reddit.com&quot;&gt;reddit&lt;/a&gt;. A similar &lt;a href=&quot;http://www.randalolson.com/2013/03/15/a-data-driven-guide-to-creating-successful-reddit-posts/&quot;&gt;analysis&lt;/a&gt; of reddit was pretty interesting, so I wanted to explore how Hacker News would compare (albeit with a more exploratory approach).&lt;/p&gt;

&lt;p&gt;Of note: I did this entirely for fun; don’t take it too seriously.&lt;/p&gt;

&lt;h2 id=&quot;when-are-posts-written&quot;&gt;When are posts written?&lt;/h2&gt;

&lt;p&gt;More items are posted on weekdays, generally around midday (EST). This is consistent with my expectations – that most people who post on Hacker News are doing so from the US and Europe. 
&lt;img src=&quot;/assets/posts.png&quot; alt=&quot;Plot of posts by day&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, if we overlay each day on the same scale, we can see that posts occur later on weekends and continue into the night. I.e., I’m not the only one who’s wasted a Saturday night on Hacker News!
&lt;img src=&quot;/assets/posts_hour.png&quot; alt=&quot;Plot of posts by time&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In terms of popularity of posts, there’s little relationship between time of day and popularity.
&lt;img src=&quot;/assets/points.png&quot; alt=&quot;Plot of average points vs time&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You may have noticed that there is a relationship between day of week and popularity of the posts. This relationship is statistically signifiant (using a non-parametric &lt;a href=&quot;http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;&gt;Mann-Whitney U Test&lt;/a&gt;) and I think more clear using boxplots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/points_day.png&quot; alt=&quot;Plot of points by day&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-posted&quot;&gt;What is posted?&lt;/h2&gt;
&lt;p&gt;One of the most interesting things to me about Hacker News is the diversity of posts. Below is a plot that shows how frequently a post comes from any site in each month. A couple slight caveats though: I only included sites that accounted for ≥2% of the posts in a month (everything else is in the ‘Other’ category), and I foolishly included ‘co.uk’ as it’s own site. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/diversity.png&quot; alt=&quot;Plot of sources&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is pretty clear that while there is a tech focus (techcrunch, ycombinator, wired, etc), no single site accounts for more than a tiny portion of the posts. I repeated this analysis for high scoring posts (&amp;gt; 100 points) and found essentially the same result. For comparison, roughly 60% of posts on reddit link to imgur. &lt;/p&gt;

&lt;p&gt;I did a similar analysis of the users who submitted posts and high-scoring posts and again found very limited concentration.&lt;/p&gt;

&lt;p&gt;Now, on to topics. To begin, I wanted to see whether it was possible to categorize topics based only on their title. Of course, I first made a wordcloud of the titles (very important step in the scientific process).
&lt;img src=&quot;/assets/wordcloud_all.png&quot; alt=&quot;Wordcloud of titles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found that, using a k-means clustering algorithm (an unsupervised learning technique) on the overlapping words, I was able to create categories that seemed pretty meaningful. However, I found that these categories were not robust; i.e., when I reran the clustering algorithm I’d get new categories. The few that were fairly consistent were: Show/Ask HN, Google, and Windows/Microsoft. A few of the categories shown below are pretty interesting: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The ‘attack’ category includes both ‘nuclear’ and ‘ddos’. &lt;/li&gt;
  &lt;li&gt;The ‘color’ category includes both ‘blind’ and ‘palette’&lt;/li&gt;
  &lt;li&gt;The ‘day’ category includes ‘zero’ and ‘demo’&lt;/li&gt;
  &lt;li&gt;There’s an entire category for ‘love tablet’. It is best not to examine this too much.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/wordcloud_25.png&quot; alt=&quot;Wordcloud of categories&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This shouldn’t be too surprising. Working with very short ‘documents’ means that coincident single words will define entire categories. This technique works better when you can compare longer documents. &lt;/p&gt;

&lt;p&gt;More seriously, in the future I’d like to see whether I can find a better way to categorize the posts, likely using the content on page they link to (difficult) or comments on HN (more doable).&lt;/p&gt;

&lt;p&gt;I then wanted to see whether any words were strong predictors of high/low scores. To do so, I ran linear regressions on the number of points for each post, and the presence of each word in that post. For example, I would look for a correlation between the number of uses of the word ‘google’ in a title and the points that post scored. Because the number of uses in the word is generally binary (either 0 or 1), this is roughly the same as asking “is the average score higher for titles using this word than for titles not using this word?”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/word_scatterplot.png&quot; alt=&quot;Scatterplot of words vs score&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I then filtered these out to exclude uncommon words and words with an insignificant impact on the total point scored and found the following.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/significant_words.png&quot; alt=&quot;Expected increase in score by word&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So using words like ‘iphone’ and ‘hn’ are correlated to lower scores, but words like ‘2048’ and ‘broke’ correspond to higher point totals. Apparently using ‘hn’ &lt;a href=&quot;https://news.ycombinator.com/item?id=7988216&quot;&gt;three times&lt;/a&gt; in &lt;a href=&quot;https://news.ycombinator.com/item?id=8561849&quot;&gt;one title&lt;/a&gt; didn’t help improve scores (or maybe these posts were just a little too meta).&lt;/p&gt;

&lt;h2 id=&quot;concluding-thoughts&quot;&gt;Concluding thoughts&lt;/h2&gt;
&lt;p&gt;While none of the findings here are all that important, I think they are rather interesting. Especially the last few items: the (in)ability to classify only using the title and the relative importance of some titles over others.&lt;/p&gt;

&lt;p&gt;More broadly, lots of these same tactics can be useful for more serious work, and it’s nice to have a data set on which to practice.&lt;/p&gt;
</content>
 </entry>
 

</feed>
